{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JSE1GGPIWZ7O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "YIjObkdDaK12",
        "outputId": "d31282d0-0cc3-4ac7-8728-d1bf87705fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Num GPUs Available:  1\n",
            "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjBuA3YNHmuL"
      },
      "source": [
        "# Model Defination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cymQbL2a3POv"
      },
      "source": [
        "## Layer Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RdgmpgpVdOSx"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.sep1 = tf.keras.layers.SeparableConv2D(int(filters / 4), [5,5], strides=[1,1], padding='same', activation='relu')\n",
        "    self.sep2 = tf.keras.layers.SeparableConv2D(filters, [5,5], strides=[1,1], padding='same')\n",
        "\n",
        "  def call(self, input):\n",
        "    output = self.sep1(input)\n",
        "    output = self.sep2(output)\n",
        "\n",
        "    return output + input\n",
        "\n",
        "class Downsample(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters):\n",
        "    super(Downsample, self).__init__()\n",
        "\n",
        "    self.sep1 = tf.keras.layers.SeparableConv2D(int(filters / 4), [5,5], strides=[2,2], padding='same', activation='relu')\n",
        "    self.sep2 = tf.keras.layers.SeparableConv2D(filters, [5,5], strides=[1,1], padding='same')\n",
        "    self.sep3 = tf.keras.layers.SeparableConv2D(filters, [3,3], strides=[2,2], padding='same')\n",
        "  \n",
        "  def call(self, input):\n",
        "    out1 = self.sep1(input)\n",
        "    out1 = self.sep2(out1)\n",
        "\n",
        "    out2 = self.sep3(input)\n",
        "    \n",
        "    return out1 + out2\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.sep1 = tf.keras.layers.SeparableConv2D(filters, [3,3], strides=[1,1], padding='same', activation='relu')\n",
        "    self.sep2 = tf.keras.layers.SeparableConv2D(filters, [3,3], strides=[1,1], padding='same')\n",
        "\n",
        "  def call (self, input):\n",
        "    out1 = self.sep1(input)\n",
        "    out1 = self.sep2(out1)\n",
        "\n",
        "    return out1 + input\n",
        "\n",
        "class Upsample(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters):\n",
        "    super(Upsample, self).__init__()\n",
        "    self.deconv = tf.keras.layers.Conv2DTranspose(filters, (3,3), strides=2, padding='same')\n",
        "\n",
        "  def call (self, input):\n",
        "    output = self.deconv(input)\n",
        "\n",
        "    return output  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqfW9p6u3VSM"
      },
      "source": [
        "## Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4PGAYFQ4ruG6"
      },
      "outputs": [],
      "source": [
        "class DenoiseNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DenoiseNetwork, self).__init__()\n",
        "\n",
        "    # Input stage\n",
        "    self.input_stage = tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", input_shape=(160, 120, 1))\n",
        "\n",
        "    # Encoder Stage 1\n",
        "    self.encoder_stage1 = tf.keras.Sequential([\n",
        "        Downsample(64),\n",
        "        Encoder(64)                       \n",
        "    ])\n",
        "\n",
        "    # Encoder Stage 2\n",
        "    self.encoder_stage2 = tf.keras.Sequential([\n",
        "        Downsample(128),\n",
        "        Encoder(128)                       \n",
        "    ])\n",
        "\n",
        "    # Encoder Stage 3\n",
        "    self.encoder_stage3 = tf.keras.Sequential([\n",
        "        Downsample(256),\n",
        "        Encoder(256),\n",
        "        Encoder(256),\n",
        "        Encoder(256)                      \n",
        "    ])\n",
        "\n",
        "    # Encoder Stage 4\n",
        "    self.encoder_stage4 = tf.keras.Sequential([\n",
        "        Downsample(512),\n",
        "        Encoder(512),\n",
        "        Encoder(512),\n",
        "        Encoder(512)                      \n",
        "    ])\n",
        "\n",
        "    # Decoder Stage 1\n",
        "    self.decoder_stage1 = tf.keras.Sequential([\n",
        "        Decoder(512),\n",
        "        Upsample(64)\n",
        "    ])\n",
        "\n",
        "    # Decoder Stage 2\n",
        "    self.decoder_stage2 = tf.keras.Sequential([\n",
        "        Decoder(64),\n",
        "        Upsample(32)\n",
        "    ])\n",
        "    # Decoder Stage 3\n",
        "    self.decoder_stage3 = tf.keras.Sequential([\n",
        "        Decoder(32),\n",
        "        Upsample(32)\n",
        "    ])\n",
        "    # Decoder Stage 4\n",
        "    self.decoder_stage4 = tf.keras.Sequential([\n",
        "        Decoder(32),\n",
        "        Upsample(16)\n",
        "    ])\n",
        "\n",
        "    # Separatable Convolution\n",
        "    self.sep1 = tf.keras.layers.SeparableConv2D(16, (3,3), padding='same')\n",
        "    self.sep2 = tf.keras.layers.SeparableConv2D(32, (3,3), padding='same')\n",
        "    self.sep3 = tf.keras.layers.SeparableConv2D(32, (3,3), padding='same')\n",
        "    self.sep4 = tf.keras.layers.SeparableConv2D(64, (3,3), padding='same')\n",
        "\n",
        "    # Output Stage\n",
        "    self.output_stage = tf.keras.Sequential([\n",
        "        Decoder(16),\n",
        "        tf.keras.layers.Conv2D(1, kernel_size=(3,3), padding='same')\n",
        "    ])\n",
        "\n",
        "  def call(self, input):\n",
        "\n",
        "    out_i = self.input_stage(input)\n",
        "    out_e1 = self.encoder_stage1(out_i)\n",
        "    out_e2 = self.encoder_stage2(out_e1)\n",
        "    out_e3 = self.encoder_stage3(out_e2)\n",
        "    out_e4 = self.encoder_stage4(out_e3)\n",
        "\n",
        "    out_d1 = self.decoder_stage1(out_e4) + self.sep4(out_e3)\n",
        "    out_d2 = self.decoder_stage2(out_d1) + self.sep3(out_e2)\n",
        "    out_d3 = self.decoder_stage3(out_d2) + self.sep2(out_e1)\n",
        "    out_d4 = self.decoder_stage4(out_d3) + self.sep1(out_i)\n",
        "    output = self.output_stage(out_d4) + input\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "846HKzeL3Y1b"
      },
      "source": [
        "## Create and Buil Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AeNUeRouiarG",
        "outputId": "88e7c08b-045a-4663-fca1-73fc1c9aeb86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = DenoiseNetwork()\n",
        "model.build((None, 160, 120, 1))\n",
        "len(model.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJZIFfhA0AWv"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HugjCsFnQgb-"
      },
      "outputs": [],
      "source": [
        "def loss_function(y_ground_truth, y_pred):\n",
        "  return tf.reduce_sum(tf.reduce_mean(tf.abs(y_pred - y_ground_truth), axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z_lEFP2oKdG9"
      },
      "outputs": [],
      "source": [
        "# Custom training step\n",
        "def train_step(model, optimizer, clean_img, noisy_img):\n",
        "\n",
        "  variables = model.variables\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    \n",
        "    tape.watch(variables)\n",
        "    y_pred = model(noisy_img)\n",
        "    loss = loss_function(clean_img, y_pred)\n",
        "\n",
        "  grads = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(grads, variables))\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Progress bar function\n",
        "def progress(epoch, batch, total_batches, trained_samples, total_samples, current_loss, avg_loss, learning_rate, bar_length=30, time_elapsed=0, message=\"\"):\n",
        "    # Calculate progress percentage\n",
        "    percent = float(trained_samples) / float(total_samples)\n",
        "    \n",
        "    # Create progress bar\n",
        "    hashes = '#' * int(round(percent * bar_length))\n",
        "    spaces = ' ' * (bar_length - len(hashes))\n",
        "\n",
        "    # Estimate time remaining\n",
        "    if time_elapsed > 0 and percent > 0:\n",
        "        time_per_sample = time_elapsed / trained_samples\n",
        "        remaining_time = time_per_sample * (total_samples - trained_samples)\n",
        "        time_remaining = f\"{int(remaining_time // 60)}m {int(remaining_time % 60)}s\"\n",
        "    else:\n",
        "        time_remaining = \"Calculating...\"\n",
        "\n",
        "    # Progress bar display\n",
        "    sys.stdout.write(\n",
        "        f\"\\rEpoch {epoch} [{batch}/{total_batches}] [{hashes}{spaces}] {int(percent * 100)}% \"\n",
        "        f\"Loss: {current_loss:.4f} / Avg Loss: {avg_loss:.4f} | \"\n",
        "        f\"LR: {learning_rate:.6f} | ETA: {time_remaining} {message}\"\n",
        "    )\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "def fit(model, lr, clean_imgs, noisy_imgs, epochs, validation_data=None, verbose=1):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [] if validation_data else None\n",
        "    }\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "    # Prepare dataset\n",
        "    dataset = tf.data.Dataset.zip((clean_imgs, noisy_imgs))\n",
        "\n",
        "    # Validation dataset (optional)\n",
        "    val_dataset = None\n",
        "    if validation_data:\n",
        "        val_clean_imgs, val_noisy_imgs = validation_data\n",
        "        val_dataset = tf.data.Dataset.zip((val_clean_imgs, val_noisy_imgs))\n",
        "\n",
        "    # Training loop\n",
        "    for e in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Iterate over batches\n",
        "        for ite, (clean_img_batch, noisy_img_batch) in enumerate(dataset):\n",
        "            batch_start_time = time.time()\n",
        "            loss = train_step(model, optimizer, clean_img_batch, noisy_img_batch)\n",
        "            total_loss += loss\n",
        "            num_batches += 1\n",
        "\n",
        "            avg_loss = total_loss / num_batches\n",
        "            trained_samples = (ite + 1) * clean_img_batch.shape[0]\n",
        "            total_samples = len(clean_imgs)\n",
        "\n",
        "            # Display progress only if verbose is 1\n",
        "            if verbose == 1:\n",
        "                progress(\n",
        "                    epoch=e+1,\n",
        "                    batch=ite+1,\n",
        "                    total_batches=len(dataset),\n",
        "                    trained_samples=trained_samples,\n",
        "                    total_samples=total_samples,\n",
        "                    current_loss=float(loss),\n",
        "                    avg_loss=float(avg_loss),\n",
        "                    learning_rate=optimizer.learning_rate.numpy(),\n",
        "                    bar_length=30,\n",
        "                    time_elapsed=(time.time() - start_time)\n",
        "                )\n",
        "\n",
        "            # Delay to simulate time taken per batch (optional)\n",
        "            time.sleep(0.01)\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        history['train_loss'].append(float(avg_loss))\n",
        "\n",
        "        if verbose >= 1:\n",
        "            print(f\"\\nEpoch {e+1}: --- Avg Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation step (if validation data is provided)\n",
        "        if val_dataset:\n",
        "            val_loss = 0.0\n",
        "            val_batches = 0\n",
        "            for val_clean_batch, val_noisy_batch in val_dataset:\n",
        "                y_val_pred = model(val_noisy_batch, training=False)\n",
        "                val_loss += loss_function(val_clean_batch, y_val_pred)\n",
        "                val_batches += 1\n",
        "\n",
        "            avg_val_loss = val_loss / val_batches\n",
        "            history['val_loss'].append(float(avg_val_loss))\n",
        "\n",
        "            if verbose >= 1:\n",
        "                print(f\"Epoch {e+1}: --- Avg Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVzy48202S5w"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the zip file\n",
        "zip_file_path = '/content/drive/My Drive/Data/FLIR_Thermal_Images_Dataset.zip'\n",
        "# Define the directory to extract the files to\n",
        "extract_dir = '/content/FLIR_Thermal_Images_Dataset'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Define the path for training, validation and test data\n",
        "train_clean_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Train/Clean'\n",
        "train_noisy_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Train/Noisy'\n",
        "\n",
        "val_clean_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Validation/Clean'\n",
        "val_noisy_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Validation/Noisy'\n",
        "\n",
        "test_clean_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Test/Clean'\n",
        "test_noisy_path = '/content/FLIR_Thermal_Images_Dataset/FLIR/Test/Noisy'\n",
        "\n",
        "# load the images\n",
        "def load_images(path):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_png(img, channels=1)\n",
        "  img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Create two datasets from images\n",
        "def create_dataset(clean_path, noisy_path, batch_size):\n",
        "  clean_files = tf.data.Dataset.list_files(clean_path + '/*.png', shuffle=False)\n",
        "  noisy_files = tf.data.Dataset.list_files(noisy_path + '/*.png', shuffle=False)\n",
        "\n",
        "  clean_dataset = clean_files.map(lambda x: load_images(x)).batch(batch_size)\n",
        "  noisy_dataset = noisy_files.map(lambda x: load_images(x)).batch(batch_size)\n",
        "\n",
        "  return clean_dataset, noisy_dataset\n",
        "\n",
        "# Create datasets for training, validation and test\n",
        "train_clean_imgs, train_noisy_imgs = create_dataset(train_clean_path, train_noisy_path, BATCH_SIZE)\n",
        "val_clean_imgs, val_noisy_imgs = create_dataset(val_clean_path, val_noisy_path, BATCH_SIZE)\n",
        "test_clean_imgs, test_noisy_imgs = create_dataset(test_clean_path, test_noisy_path, BATCH_SIZE)\n",
        "\n",
        "print(\"Training Clean Images: \", len(train_clean_imgs))\n",
        "print(\"Training Noisy Images: \", len(train_noisy_imgs))\n",
        "print(\"Validation Clean Images: \", len(val_clean_imgs))\n",
        "print(\"Validation Noisy Images: \", len(val_noisy_imgs))\n",
        "print(\"Test Clean Images: \", len(test_clean_imgs))\n",
        "print(\"Test Noisy Images: \", len(test_noisy_imgs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBy7ahXB3gFe"
      },
      "source": [
        "## Train with first 30 epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "z9zH6KwP2Brk",
        "outputId": "1ee23059-64d6-4c95-dad0-48b87ed5daf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: [#########################] 100%  ----- Loss: 6180.927734375\n",
            " Epoch 1: --- Avg Loss: 6680.73388671875\n",
            "Epoch 2: [#########################] 100%  ----- Loss: 6098.9521484375\n",
            " Epoch 2: --- Avg Loss: 4344.837890625\n",
            "Epoch 3: [#########################] 100%  ----- Loss: 6162.5419921875\n",
            " Epoch 3: --- Avg Loss: 4040.492919921875\n",
            "Epoch 4: [#########################] 100%  ----- Loss: 6165.201171875\n",
            " Epoch 4: --- Avg Loss: 3856.072509765625\n",
            "Epoch 5: [#########################] 100%  ----- Loss: 6453.93896484375\n",
            " Epoch 5: --- Avg Loss: 3674.99267578125\n",
            "Epoch 6: [#########################] 100%  ----- Loss: 6257.0380859375\n",
            " Epoch 6: --- Avg Loss: 3601.1455078125\n",
            "Epoch 7: [#########################] 100%  ----- Loss: 5604.1357421875\n",
            " Epoch 7: --- Avg Loss: 3342.703857421875\n",
            "Epoch 8: [#########################] 100%  ----- Loss: 5400.5791015625\n",
            " Epoch 8: --- Avg Loss: 3162.960693359375\n",
            "Epoch 9: [#########################] 100%  ----- Loss: 5147.63134765625\n",
            " Epoch 9: --- Avg Loss: 3018.81103515625\n",
            "Epoch 10: [#########################] 100%  ----- Loss: 4568.04931640625\n",
            " Epoch 10: --- Avg Loss: 2877.83642578125\n",
            "Epoch 11: [#########################] 100%  ----- Loss: 3720.44677734375\n",
            " Epoch 11: --- Avg Loss: 2737.471435546875\n",
            "Epoch 12: [#########################] 100%  ----- Loss: 3133.021240234375\n",
            " Epoch 12: --- Avg Loss: 2654.349609375\n",
            "Epoch 13: [#########################] 100%  ----- Loss: 2649.0712890625\n",
            " Epoch 13: --- Avg Loss: 2570.5673828125\n",
            "Epoch 14: [#########################] 100%  ----- Loss: 2430.510009765625\n",
            " Epoch 14: --- Avg Loss: 2502.602294921875\n",
            "Epoch 15: [#########################] 100%  ----- Loss: 2393.71826171875\n",
            " Epoch 15: --- Avg Loss: 2414.421142578125\n",
            "Epoch 16: [#########################] 100%  ----- Loss: 2557.55224609375\n",
            " Epoch 16: --- Avg Loss: 2406.061767578125\n",
            "Epoch 17: [#########################] 100%  ----- Loss: 2417.47607421875\n",
            " Epoch 17: --- Avg Loss: 2369.104736328125\n",
            "Epoch 18: [#########################] 100%  ----- Loss: 2125.775390625\n",
            " Epoch 18: --- Avg Loss: 2306.947265625\n",
            "Epoch 19: [#########################] 100%  ----- Loss: 2058.5888671875\n",
            " Epoch 19: --- Avg Loss: 2244.503662109375\n",
            "Epoch 20: [#########################] 100%  ----- Loss: 2032.677490234375\n",
            " Epoch 20: --- Avg Loss: 2192.80908203125\n"
          ]
        }
      ],
      "source": [
        "history = fit(\n",
        "    model=model,\n",
        "    lr=1e-3,\n",
        "    clean_imgs=train_clean_imgs,\n",
        "    noisy_imgs=train_noisy_imgs,\n",
        "    epochs=30,\n",
        "    validation_data=(val_clean_imgs, val_noisy_imgs),  # Optional validation data\n",
        "    verbose=1  # Progress bar\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "DJ6mTj6n7_uq",
        "outputId": "2c355201-dc4c-4ebd-9b0e-f93c833e55db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb5de33df28>]"
            ]
          },
          "execution_count": 126,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.plot(epochs, history['train_loss'], 'r-', label='Training Loss', marker='o')\n",
        "\n",
        "    # If validation loss exists, plot it\n",
        "    if 'val_loss' in history:\n",
        "        plt.plot(epochs, history['val_loss'], 'b-', label='Validation Loss', marker='x')\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "\n",
        "    # Add grid for better readability\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gfuP3yTr2P41",
        "outputId": "a9ada315-10d5-47f4-f092-70b7fc717e84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbe7cc7f630>"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the entir model as a TensorFlow SavedModel\n",
        "# This will save weights and architecture\n",
        "model.save('/content/drive/My Drive/Models/pmrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TensorFlow Lite For Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/Models/pmrid')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model as a .tflite file\n",
        "with open('/content/drive/My Drive/Models/pmrid.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mVLxnCwm679M"
      ],
      "name": "Deep Raw Image Denoising.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
